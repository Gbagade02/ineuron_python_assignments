{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f480d-2199-440d-a700-bb1e5aa4a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Define the Bayesian interpretation of probability.\n",
    "2. Define probability of a union of two events with equation.\n",
    "3. What is joint probability? What is its formula?\n",
    "4. What is chain rule of probability?\n",
    "5. What is conditional probability means? What is the formula of it?\n",
    "6. What are continuous random variables?\n",
    "7. What are Bernoulli distributions? What is the formula of it?\n",
    "8. What is binomial distribution? What is the formula?\n",
    "9. What is Poisson distribution? What is the formula?\n",
    "10. Define covariance.\n",
    "11. Define correlation\n",
    "12. Define sampling with replacement. Give example.\n",
    "13. What is sampling without replacement? Give example.\n",
    "14. What is hypothesis? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc9a31-a2f7-4e3a-9bbd-050998b6904c",
   "metadata": {},
   "source": [
    "## 1. Define the Bayesian interpretation of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82432238-75f8-4d12-ac11-1d2a86dba15a",
   "metadata": {},
   "source": [
    "The Bayesian interpretation of probability is a philosophical and mathematical framework that views probability as a measure of belief or uncertainty in a proposition or event. It is named after Thomas Bayes, an 18th-century mathematician and theologian.\n",
    "\n",
    "In the Bayesian interpretation, probability is subjective and represents an individual's degree of confidence or subjective belief about the truth of a statement, given the available evidence or prior knowledge. It provides a way to update beliefs based on new evidence through a process known as Bayesian inference.\n",
    "\n",
    "At the core of the Bayesian interpretation is Bayes' theorem, which mathematically describes how prior beliefs can be updated in light of new evidence. The theorem states that the posterior probability of an event or hypothesis (the updated belief) is proportional to the prior probability (initial belief) multiplied by the likelihood of the evidence given the event/hypothesis.\n",
    "\n",
    "Bayes' theorem can be expressed as follows:\n",
    "\n",
    "    P(H|E) = (P(E|H) * P(H)) / P(E)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(H|E) represents the posterior probability of hypothesis H given evidence E.\n",
    "- P(E|H) is the likelihood of evidence E given hypothesis H.\n",
    "- P(H) is the prior probability of hypothesis H.\n",
    "- P(E) is the probability of evidence E.\n",
    "\n",
    "The Bayesian interpretation allows for the incorporation of prior knowledge or beliefs into the analysis and the continuous updating of probabilities as new evidence becomes available. It provides a flexible and intuitive framework for reasoning under uncertainty, making it widely applicable in various fields, including statistics, machine learning, and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20d423-3d21-48f7-bbca-bfe968c67af3",
   "metadata": {},
   "source": [
    "### 2. Define probability of a union of two events with equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c4678-b6e1-47dd-b2f8-fb1b85dff044",
   "metadata": {},
   "source": [
    "The probability of the union of two events A and B, denoted as P(A ∪ B), represents the probability that at least one of the events A or B occurs. The equation for calculating the probability of the union of two events depends on whether the events are mutually exclusive or not.\n",
    "\n",
    "1. Mutually Exclusive Events:\n",
    "If events A and B are mutually exclusive, it means that they cannot occur simultaneously. In this case, the equation for the probability of their union is simply the sum of their individual probabilities:\n",
    "\n",
    "    P(A ∪ B) = P(A) + P(B)\n",
    "\n",
    "For example, if A represents the event of rolling an even number on a fair six-sided die, and B represents the event of rolling an odd number, the probability of their union (rolling an even number or an odd number) would be:\n",
    "    \n",
    "    P(A ∪ B) = P(A) + P(B) = 3/6 + 3/6 = 1/2 + 1/2 = 1\n",
    "\n",
    "2. Non-Mutually Exclusive Events:\n",
    "If events A and B are not mutually exclusive, meaning they can occur simultaneously, the equation for the probability of their union needs to account for the possibility of double-counting the overlapping region (the intersection of A and B). The equation is:\n",
    "\n",
    "    P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\n",
    "\n",
    "Here, P(A ∩ B) represents the probability of the intersection of events A and B.\n",
    "\n",
    "For example, if A represents the event of rolling an even number on a fair six-sided die, and B represents the event of rolling a number less than 4, the probability of their union (rolling an even number or a number less than 4) would be:\n",
    "\n",
    "    P(A ∪ B) = P(A) + P(B) - P(A ∩ B) = 3/6 + 3/6 - 2/6 = 4/6 = 2/3\n",
    "\n",
    "In this case, the probability of rolling an even number or a number less than 4 is 2/3, as rolling a 2 or a 6 would be counted twice (once in each event), so it needs to be subtracted once to avoid double-counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722cfb5-9b3a-4abf-879f-7699f74dcaaf",
   "metadata": {},
   "source": [
    "### 3. What is joint probability? What is its formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b05f76-3ef8-4fe2-9dd4-3318f57e67a5",
   "metadata": {},
   "source": [
    "Joint probability refers to the probability of two or more events occurring simultaneously or in conjunction with each other. It represents the likelihood of the intersection of multiple events. The joint probability of events A and B is denoted as P(A ∩ B).\n",
    "\n",
    "The formula for calculating the joint probability of two events depends on whether the events are independent or dependent.\n",
    "\n",
    "1. Independent Events:\n",
    "If events A and B are independent, it means that the occurrence of one event does not affect the probability of the other event. In this case, the formula for joint probability is:\n",
    "\n",
    "    P(A ∩ B) = P(A) * P(B)\n",
    "\n",
    "For example, if A represents the event of flipping heads on a fair coin, and B represents the event of rolling a 6 on a fair six-sided die, the joint probability of both events occurring (getting heads and rolling a 6) would be:\n",
    "\n",
    "    P(A ∩ B) = P(A) * P(B) = 1/2 * 1/6 = 1/12\n",
    "\n",
    "2. Dependent Events:\n",
    "If events A and B are dependent, it means that the occurrence of one event affects the probability of the other event. In this case, the formula for joint probability is:\n",
    "\n",
    "    P(A ∩ B) = P(A) * P(B|A)\n",
    "\n",
    "Here, P(B|A) represents the conditional probability of event B given that event A has occurred.\n",
    "\n",
    "For example, if A represents the event of drawing a red card from a standard deck of 52 playing cards, and B represents the event of drawing a king from the remaining cards after one card has already been drawn, the joint probability of both events occurring (drawing a red card and then drawing a king) would be:\n",
    "\n",
    "    P(A ∩ B) = P(A) * P(B|A) = (26/52) * (4/51) = 1/52\n",
    "\n",
    "In this case, the probability of drawing a red card and then drawing a king is 1/52, as the probability of drawing a king depends on the fact that a red card has already been drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da6ad9-c4bf-428c-a203-d46c0463fc5e",
   "metadata": {},
   "source": [
    "### 4. What is chain rule of probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3f045-aba9-46fa-8128-b057861db7ff",
   "metadata": {},
   "source": [
    "The chain rule of probability, also known as the multiplication rule, is a fundamental principle in probability theory that allows us to calculate the probability of a series of events occurring together. It enables us to express the joint probability of multiple events in terms of conditional probabilities.\n",
    "\n",
    "The chain rule of probability states that the joint probability of multiple events can be calculated by multiplying the conditional probabilities of each event given the previous events in the chain.\n",
    "\n",
    "Mathematically, the chain rule can be expressed as follows:\n",
    "\n",
    "    P(A1, A2, ..., An) = P(A1) * P(A2|A1) * P(A3|A1, A2) * ... * P(An|A1, A2, ..., An-1)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(A1, A2, ..., An) represents the joint probability of events A1, A2, ..., An occurring together.\n",
    "- P(A1) is the probability of the first event A1.\n",
    "- P(A2|A1) is the conditional probability of event A2 given that event A1 has occurred.\n",
    "- P(A3|A1, A2) is the conditional probability of event A3 given that events A1 and A2 have occurred.\n",
    "- P(An|A1, A2, ..., An-1) is the conditional probability of event An given that events A1, A2, ..., An-1 have occurred.\n",
    "\n",
    "The chain rule is particularly useful when dealing with complex scenarios involving multiple dependent events. By breaking down the joint probability into a product of conditional probabilities, we can compute the overall probability step-by-step, incorporating the dependencies between the events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de65714-2f46-40c4-a0bf-a68c00acd22c",
   "metadata": {},
   "source": [
    "### 5. What is conditional probability means? What is the formula of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad51875-d737-43bb-a3b9-be6408b836ee",
   "metadata": {},
   "source": [
    "Conditional probability refers to the probability of an event occurring given that another event has already occurred. It measures the likelihood of an event happening, taking into account the knowledge or information about the occurrence of another event.\n",
    "\n",
    "The conditional probability of event A given event B is denoted as P(A|B), read as \"the probability of A given B.\" It represents the probability of event A occurring, given the information that event B has already occurred.\n",
    "\n",
    "The formula for calculating conditional probability is derived from the definition of probability and can be expressed as:\n",
    "\n",
    "    P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(A|B) represents the conditional probability of event A given event B.\n",
    "- P(A ∩ B) is the joint probability of events A and B occurring together.\n",
    "- P(B) is the probability of event B occurring.\n",
    "\n",
    "In words, the formula states that the conditional probability of A given B is equal to the joint probability of A and B divided by the probability of B.\n",
    "\n",
    "The formula highlights the relationship between the joint probability and the conditional probability. It allows us to adjust the probability of event A based on the additional information provided by the occurrence of event B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2f6c7-2ee1-40ab-9ff4-6d57c6e698b7",
   "metadata": {},
   "source": [
    "### 6. What are continuous random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac0e10-81e1-4255-bc84-6a932319a814",
   "metadata": {},
   "source": [
    "Continuous random variables are variables that can take on any value within a specified range or interval. They are defined on continuous sample spaces and can assume an infinite number of possible values within their range. Examples of continuous random variables include time, height, temperature, and weight.\n",
    "\n",
    "Unlike discrete random variables, which can only take on a countable set of distinct values, continuous random variables can take on any value within a given interval. This is because continuous variables are associated with measurements or quantities that can be expressed with real numbers and can be infinitely subdivided.\n",
    "\n",
    "The probability distribution of a continuous random variable is described by a probability density function (PDF) instead of a probability mass function (PMF) as seen in discrete random variables. The PDF represents the probability of the variable taking on a specific value or falling within a certain range of values. The total area under the PDF curve over the entire range of possible values is equal to 1.\n",
    "\n",
    "When working with continuous random variables, probabilities are calculated by determining the area under the PDF curve corresponding to a specific range of values. For example, to find the probability that a continuous random variable falls between two specific values, you would integrate the PDF over that range.\n",
    "\n",
    "Continuous random variables are commonly used in various fields such as statistics, physics, finance, and engineering, where measurements and quantities are often represented by real numbers and can vary continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be207b2d-01ab-4d78-86dc-d9e04a97b9f2",
   "metadata": {},
   "source": [
    "### 7. What are Bernoulli distributions? What is the formula of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d6eb1-6851-48ab-8cd5-dd7516f41794",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a single trial with two possible outcomes: success (typically denoted as \"1\") and failure (typically denoted as \"0\"). It is named after Jacob Bernoulli, a Swiss mathematician who introduced the concept in the 18th century.\n",
    "\n",
    "The Bernoulli distribution is characterized by a single parameter, often denoted as \"p,\" which represents the probability of success in a single trial. The probability mass function (PMF) of the Bernoulli distribution is given by the following formula:\n",
    "\n",
    "    P(X = x) = p^x * (1 - p)^(1-x)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(X = x) represents the probability of the random variable X taking on a specific value x (either 0 or 1) in a single trial.\n",
    "- p is the probability of success in a single trial.\n",
    "- x is the value of the random variable X, which can be either 0 (failure) or 1 (success).\n",
    "\n",
    "The PMF formula indicates that the probability of success (X = 1) is p, while the probability of failure (X = 0) is (1 - p).\n",
    "\n",
    "The expected value or mean of a Bernoulli distribution is given by:\n",
    "\n",
    "    E(X) = p\n",
    "\n",
    "This means that the average outcome or expected proportion of successes in a series of Bernoulli trials with the same success probability p is equal to p.\n",
    "\n",
    "The variance of a Bernoulli distribution is given by:\n",
    "\n",
    "    Var(X) = p * (1 - p)\n",
    "\n",
    "The variance measures the spread or variability of the distribution.\n",
    "\n",
    "The Bernoulli distribution is often used as a building block for more complex distributions and models, such as the binomial distribution, which represents the number of successes in a fixed number of independent Bernoulli trials. It also serves as a foundation for understanding other important concepts in probability theory and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8fad84-ff7d-4f7f-ac20-94d05856e1c8",
   "metadata": {},
   "source": [
    "### 8. What is binomial distribution? What is the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9924987-5308-4784-98c5-1938c644a050",
   "metadata": {},
   "source": [
    "The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials. It is commonly used when there are only two possible outcomes in each trial, typically labeled as success (usually denoted as \"1\") and failure (usually denoted as \"0\").\n",
    "\n",
    "The binomial distribution is characterized by two parameters: the number of trials, denoted as \"n,\" and the probability of success in a single trial, denoted as \"p.\" The probability mass function (PMF) of the binomial distribution is given by the following formula:\n",
    "\n",
    "    P(X = k) = C(n, k) * p^k * (1 - p)^(n-k)\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(X = k) represents the probability of the random variable X taking on a specific value k (where k can range from 0 to n) in n independent trials.\n",
    "- C(n, k) represents the number of combinations of n items taken k at a time, often referred to as \"n choose k\" or the binomial coefficient. It can be calculated as C(n, k) = n! / (k! * (n - k)!), where \"!\" denotes factorial.\n",
    "- p is the probability of success in a single trial.\n",
    "- k is the number of successes in the n trials.\n",
    "\n",
    "The binomial distribution formula calculates the probability of getting exactly k successes in n independent trials, where each trial has a fixed success probability p.\n",
    "\n",
    "The expected value or mean of a binomial distribution is given by:\n",
    "\n",
    "    E(X) = n * p\n",
    "\n",
    "This means that the average number of successes in n independent trials, each with a success probability p, is equal to n * p.\n",
    "\n",
    "The variance of a binomial distribution is given by:\n",
    "\n",
    "    Var(X) = n * p * (1 - p)\n",
    "\n",
    "The variance measures the spread or variability of the distribution.\n",
    "\n",
    "The binomial distribution is widely used in various fields, such as statistics, genetics, finance, and quality control, to model situations where there are a fixed number of trials with a binary outcome and a known probability of success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e0bb7-f4d4-4158-9be2-ccd07ac9c7b0",
   "metadata": {},
   "source": [
    "### 9. What is Poisson distribution? What is the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3d6aa-3260-48e6-a219-73d91dc6326a",
   "metadata": {},
   "source": [
    "The Poisson distribution is a discrete probability distribution that models the number of events that occur within a fixed interval of time or space, given the average rate of occurrence. It is named after Siméon Denis Poisson, a French mathematician who introduced the concept in the early 19th century.\n",
    "\n",
    "The Poisson distribution is characterized by a single parameter, often denoted as \"λ\" (lambda), which represents the average rate of event occurrences within the given interval. The probability mass function (PMF) of the Poisson distribution is given by the following formula:\n",
    "\n",
    "    P(X = k) = (e^(-λ) * λ^k) / k!\n",
    "\n",
    "Where:\n",
    "\n",
    "- P(X = k) represents the probability of the random variable X taking on a specific value k (where k can be any non-negative integer) within the given interval.\n",
    "- λ (lambda) is the average rate of event occurrences within the interval.\n",
    "- e is Euler's number, a mathematical constant approximately equal to 2.71828.\n",
    "- k is the number of events occurring within the interval.\n",
    "\n",
    "The PMF formula calculates the probability of observing exactly k events within the given interval, based on the average rate λ.\n",
    "\n",
    "The expected value or mean of a Poisson distribution is given by:\n",
    "\n",
    "    E(X) = λ\n",
    "\n",
    "This means that the average number of events occurring within the interval, based on the average rate λ, is equal to λ.\n",
    "\n",
    "The variance of a Poisson distribution is also equal to λ:\n",
    "\n",
    "    Var(X) = λ\n",
    "\n",
    "The Poisson distribution is commonly used to model rare events or random occurrences, such as the number of phone calls received in a call center within a specific time period, the number of accidents at a particular intersection in a day, or the number of defects in a product.\n",
    "\n",
    "It is important to note that the Poisson distribution assumes that the events occur independently and at a constant average rate throughout the interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987443ab-5b96-4bf2-8ba3-343ae4f6295f",
   "metadata": {},
   "source": [
    "### 10. Define covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e1c3d-7432-492f-9dc5-efc9477951fe",
   "metadata": {},
   "source": [
    "Covariance is a statistical measure that quantifies the relationship between two random variables. It describes how changes in one variable are associated with changes in another variable. In particular, covariance measures the joint variability or co-variability between the two variables.\n",
    "\n",
    "For two random variables X and Y, the covariance between them is denoted as Cov(X, Y). It is calculated using the following formula:\n",
    "\n",
    "    Cov(X, Y) = E[(X - E(X))(Y - E(Y))]\n",
    "\n",
    "Where:\n",
    "\n",
    "- Cov(X, Y) represents the covariance between random variables X and Y.\n",
    "- E(X) and E(Y) represent the expected values (means) of X and Y, respectively.\n",
    "\n",
    "The formula computes the expected value of the product of the deviations of X and Y from their respective means. In other words, it measures how much the values of X and Y deviate from their means together.\n",
    "\n",
    "The sign of the covariance indicates the direction of the relationship between the variables:\n",
    "\n",
    "- If Cov(X, Y) > 0, it suggests a positive covariance, indicating that when X is above its mean, Y tends to be above its mean, and vice versa.\n",
    "- If Cov(X, Y) < 0, it suggests a negative covariance, indicating that when X is above its mean, Y tends to be below its mean, and vice versa.\n",
    "- If Cov(X, Y) = 0, it suggests no linear relationship or covariance between X and Y. However, it does not necessarily mean that the variables are independent.\n",
    "\n",
    "It is important to note that the magnitude of the covariance does not provide a standardized measure of the strength of the relationship between the variables. To assess the strength and direction of the relationship, covariance is often standardized by dividing it by the product of the standard deviations of X and Y. This standardized measure is called the correlation coefficient, which provides a value between -1 and 1, indicating the strength and direction of the linear relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8097af-9815-4404-bcb2-edb8505be194",
   "metadata": {},
   "source": [
    "### 11. Define correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a89c79-3b3b-43bb-82d3-15ed77822d7d",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. It provides a standardized measure of the degree to which changes in one variable are associated with changes in another variable.\n",
    "\n",
    "Correlation is typically represented by the correlation coefficient, denoted as \"r.\" The correlation coefficient ranges from -1 to 1, where:\n",
    "\n",
    "- A correlation coefficient of +1 indicates a perfect positive correlation, meaning that as one variable increases, the other variable increases proportionally in a linear fashion.\n",
    "- A correlation coefficient of -1 indicates a perfect negative correlation, meaning that as one variable increases, the other variable decreases proportionally in a linear fashion.\n",
    "- A correlation coefficient of 0 indicates no linear correlation between the variables. It suggests that there is no linear relationship between the variables, although there could still be a non-linear relationship.\n",
    "\n",
    "The correlation coefficient is calculated using the following formula:\n",
    "\n",
    "    r = Cov(X, Y) / (σX * σY)\n",
    "\n",
    "Where:\n",
    "\n",
    "- r represents the correlation coefficient between variables X and Y.\n",
    "- Cov(X, Y) is the covariance between X and Y.\n",
    "- σX and σY are the standard deviations of X and Y, respectively.\n",
    "\n",
    "The formula standardizes the covariance by dividing it by the product of the standard deviations, resulting in a value between -1 and 1.\n",
    "\n",
    "Correlation allows us to assess the direction and strength of the linear relationship between two variables. However, it is important to note that correlation does not imply causation. A strong correlation between two variables does not necessarily indicate a cause-and-effect relationship between them; other factors and underlying relationships should be considered for causal interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184815c-0b89-4f9e-9665-125b3f69527f",
   "metadata": {},
   "source": [
    "### 12. Define sampling with replacement. Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c6c09-200d-49c7-9e45-281fe1d31464",
   "metadata": {},
   "source": [
    "Sampling with replacement is a sampling method in statistics and probability theory where, after each selection, the chosen item is returned to the population, making it available for selection again. In other words, each item in the population has an equal chance of being selected on each draw, regardless of previous selections.\n",
    "\n",
    "Here's an example to illustrate sampling with replacement:\n",
    "\n",
    "1. Suppose you have a bag with five colored balls: red, blue, green, yellow, and purple. You want to randomly select two balls from the bag using sampling with replacement.You start by selecting the first ball. Let's say you randomly pick the green ball from the bag.\n",
    "\n",
    "2. After noting the color of the first ball, you put it back into the bag, making all five balls available for the next draw.\n",
    "\n",
    "3. You repeat the process and select the second ball. This time, let's assume you pick the blue ball.\n",
    "\n",
    "With sampling with replacement, you can select the same item multiple times in different draws. In this example, you had the possibility of selecting the green ball again, even after it was chosen in the first draw.\n",
    "\n",
    "Sampling with replacement is commonly used when you want to simulate the behavior of randomly drawing items from a finite population, where the population size remains constant throughout the sampling process. It is also useful for statistical techniques like bootstrapping, which involve creating random samples for resampling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe1dd2-1796-4184-9fa9-0d4cc15698ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 13. What is sampling without replacement? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b992d1-4491-4f37-881e-a2a7911e8b12",
   "metadata": {},
   "source": [
    "### 14. What is hypothesis? Give example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
